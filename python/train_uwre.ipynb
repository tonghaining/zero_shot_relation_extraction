{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    string = re.sub(r'\\(|\\)', '', string)\n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2,\n",
    "    \"hidden\" : 0\n",
    "}\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "def load_uwre_data(path):\n",
    "    \"\"\"\n",
    "    Load UWRE data.\n",
    "    \"uwre\" is set to \"genre\".\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            loaded_example[\"genre\"] = \"uwre\"\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uwre_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"\n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['sentence1']))\n",
    "            word_counter.update(tokenize(example['sentence2']))\n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "\n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbedding_rand(path, word_indices):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings. Doing a random normal initialization for OOV words.\n",
    "    \"\"\"\n",
    "    n = len(word_indices)\n",
    "    m = 300\n",
    "    emb = np.empty((n, m), dtype=np.float32)\n",
    "\n",
    "    emb[:,:] = np.random.normal(size=(n,m))\n",
    "\n",
    "    emb[0:2, :] = np.zeros((1,m), dtype=\"float32\")\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            s = line.split()\n",
    "            if s[0] in word_indices:\n",
    "                if len(s) > 301:\n",
    "                    tail = s[len(s)-300:]                \n",
    "                    head = [s[0]]\n",
    "                    s = head + tail\n",
    "                    # print(head)\n",
    "                emb[word_indices[s[0]], :] = np.asarray(s[1:]) \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    Get true length of sequences (without padding), and mask for true-length in max-length.\n",
    "\n",
    "    Input of shape: (batch_size, max_seq_length, hidden_dim)\n",
    "    Output shapes, \n",
    "    length: (batch_size)\n",
    "    mask: (batch_size, max_seq_length, 1)\n",
    "    \"\"\"\n",
    "    populated = tf.sign(tf.abs(sequence))\n",
    "    length = tf.cast(tf.reduce_sum(populated, axis=1), tf.int32)\n",
    "    mask = tf.cast(tf.expand_dims(populated, -1), tf.float32)\n",
    "    return length, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biLSTM(inputs, dim, seq_len, name):\n",
    "    \"\"\"\n",
    "    A Bi-Directional LSTM layer. Returns forward and backward hidden states as a tuple, and cell states as a tuple.\n",
    "\n",
    "    Ouput of hidden states: [(batch_size, max_seq_length, hidden_dim), (batch_size, max_seq_length, hidden_dim)]\n",
    "    Same shape for cell states.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scope('forward' + name):\n",
    "            lstm_fwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "        with tf.variable_scope('backward' + name):\n",
    "            lstm_bwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "\n",
    "        hidden_states, cell_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fwd, cell_bw=lstm_bwd, inputs=inputs, sequence_length=seq_len, dtype=tf.float32, scope=name)\n",
    "\n",
    "    return hidden_states, cell_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(inputs, lengths, output_size, contexts=(None, None), scope=None):\n",
    "    \"\"\"Dynamic bi-LSTM reader; can be conditioned with initial state of other rnn.\n",
    "    Args:\n",
    "        inputs (tensor): The inputs into the bi-LSTM\n",
    "        lengths (tensor): The lengths of the sequences\n",
    "        output_size (int): Size of the LSTM state of the reader.\n",
    "        context (tensor=None, tensor=None): Tuple of initial (forward, backward) states\n",
    "                                  for the LSTM\n",
    "        scope (string): The TensorFlow scope for the reader.\n",
    "        drop_keep_drop (float=1.0): The keep probability for dropout.\n",
    "    Returns:\n",
    "        Outputs (tensor): The outputs from the bi-LSTM.\n",
    "        States (tensor): The cell states from the bi-LSTM.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope or \"reader\") as varscope:\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            inputs,\n",
    "            sequence_length=lengths,\n",
    "            initial_state_fw=contexts[0],\n",
    "            initial_state_bw=contexts[1],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        return outputs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uwre = load_uwre_data('./mini_uwre.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = build_uwre_dictionary([training_uwre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings = loadEmbedding_rand('./mini_glove.txt', word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(loaded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = emb_dim = 300 (embedding dimension)\n",
    "# dim = hidden_dim = 300 (hidden embedding dimension)\n",
    "# sequence_length = seq_length = 50\n",
    "embeddings = loaded_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the placeholders\n",
    "premise_x = tf.placeholder(tf.int32, [None, 50]) # sequence_length = 50\n",
    "hypothesis_x = tf.placeholder(tf.int32, [None, 50])# sequence_length = 50\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "keep_rate_ph = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 50) dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "E = tf.Variable(embeddings, True)\n",
    "\n",
    "W_mlp = tf.Variable(tf.random_normal([300 * 8, 300], stddev=0.1))\n",
    "b_mlp = tf.Variable(tf.random_normal([300], stddev=0.1))\n",
    "\n",
    "W_cl = tf.Variable(tf.random_normal([300, 3], stddev=0.1))\n",
    "b_cl = tf.Variable(tf.random_normal([3], stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for embedding lookup and dropout at embedding layer\n",
    "def emb_drop(x):\n",
    "    emb = tf.nn.embedding_lookup(E, x)\n",
    "    emb_drop = tf.nn.dropout(emb, keep_rate_ph)\n",
    "    return emb_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lengths of unpadded sentences\n",
    "prem_seq_lengths, mask_prem = length(premise_x)\n",
    "hyp_seq_lengths, mask_hyp = length(hypothesis_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_1:0' shape=(?, 50, 1) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(?, 50, 1) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First cbiLSTM layer ###\n",
    "premise_in = emb_drop(premise_x)\n",
    "hypothesis_in = emb_drop(hypothesis_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/mul:0' shape=(?, 50, 300) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sequence_length must be a vector of length batch_size, but saw shape: ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-23b1e3588af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhypothesis_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hypothesis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conditional_first_premise_layer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfstPremise_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpremise_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprem_seq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfstPremise_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-af29a20e3bce>\u001b[0m in \u001b[0;36mbiLSTM\u001b[0;34m(inputs, dim, seq_len, name)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlstm_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional_dynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_fw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_fwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_bw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_bwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    621\u001b[0m         raise ValueError(\n\u001b[1;32m    622\u001b[0m             \u001b[0;34m\"sequence_length must be a vector of length batch_size, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \"but saw shape: %s\" % sequence_length.get_shape())\n\u001b[0m\u001b[1;32m    624\u001b[0m       sequence_length = array_ops.identity(  # Just to find it in the graph.\n\u001b[1;32m    625\u001b[0m           sequence_length, name=\"sequence_length\")\n",
      "\u001b[0;31mValueError\u001b[0m: sequence_length must be a vector of length batch_size, but saw shape: ()"
     ]
    }
   ],
   "source": [
    "hypothesis_outs, c2 = biLSTM(hypothesis_in, dim=300, seq_len=50, name='hypothesis')\n",
    "with tf.variable_scope(\"conditional_first_premise_layer\") as fstPremise_scope:\n",
    "    premise_outs, c1 = reader(premise_in, prem_seq_lengths, self.dim, c2, scope=fstPremise_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
