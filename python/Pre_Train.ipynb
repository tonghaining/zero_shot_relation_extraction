{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2358,
     "status": "ok",
     "timestamp": 1564256217693,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "WBnOjzLdAPuN",
    "outputId": "9372e11f-0289-4574-bb7b-ea4d6772a5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6981,
     "status": "ok",
     "timestamp": 1564256222341,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "_gyKfdIWAxMT",
    "outputId": "4fd32810-4b24-4264-dd1a-d1d86494f832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aisin_gioro_models\t\t     glove.840B.300d.txt\n",
      "codes\t\t\t\t     model\n",
      "data\t\t\t\t     papers\n",
      "extended_relation_descriptions.json  relation_descriptions.json\n"
     ]
    }
   ],
   "source": [
    "the_path = '/content/gdrive/My Drive/master_thesis/'\n",
    "! ls gdrive/My\\ Drive/master_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjEY_fBqEAKM"
   },
   "outputs": [],
   "source": [
    "# download multiNLI data\n",
    "# ! wget \"https://www.nyu.edu/projects/bowman/multinli/multinli_0.9.zip\"\n",
    "# ! unzip multinli_0.9.zip\n",
    "# ! cp -r multinli_0.9 gdrive/My\\ Drive/master_thesis/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhO_NKvoEJZm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCLO8Um9G02f"
   },
   "outputs": [],
   "source": [
    "# logger\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    A logging that doesn't leave logs open between writes, so as to allow AFS synchronization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Level constants\n",
    "    DEBUG = 0\n",
    "    INFO = 1\n",
    "    WARNING = 2\n",
    "    ERROR = 3\n",
    "\n",
    "    def __init__(self, log_path=None, json_log_path=None, min_print_level=0, min_file_level=0):\n",
    "        \"\"\"\n",
    "        log_path: The full path for the log file to write. The file will be appended to if it exists.\n",
    "        min_print_level: Only messages with level above this level will be printed to stderr.\n",
    "        min_file_level: Only messages with level above this level will be written to disk.\n",
    "        \"\"\"\n",
    "        self.log_path = log_path\n",
    "        self.json_log_path = json_log_path\n",
    "        self.min_print_level = min_print_level\n",
    "        self.min_file_level = min_file_level\n",
    "\n",
    "    def Log(self, message, level=INFO):\n",
    "        if level >= self.min_print_level:\n",
    "            # Write to STDERR\n",
    "            sys.stderr.write(\"[%i] %s\\n\" % (level, message))\n",
    "        if self.log_path and level >= self.min_file_level:\n",
    "            # Write to the log file then close it\n",
    "            with open(self.log_path, 'a') as f:\n",
    "                datetime_string = datetime.datetime.now().strftime(\n",
    "                    \"%y-%m-%d %H:%M:%S\")\n",
    "                f.write(\"%s [%i] %s\\n\" % (datetime_string, level, message))\n",
    "\n",
    "    def LogJSON(self, message_obj, level=INFO):\n",
    "        if self.json_log_path and level >= self.min_file_level:\n",
    "            with open(self.json_log_path, 'w') as f:\n",
    "                print >>f, json.dumps(message_obj)\n",
    "        else:\n",
    "            sys.stderr.write('WARNING: No JSON log filename.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZUr2Qeika69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bU57leJsiRzU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "NLI_LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"contradiction\": 1,\n",
    "    \"hidden\": 0\n",
    "}\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 1,\n",
    "    \"hidden\": 0\n",
    "}\n",
    "\n",
    "# parameter\n",
    "\n",
    "FIXED_PARAMETERS = {}\n",
    "\n",
    "FIXED_PARAMETERS[\"seq_length\"] = 50\n",
    "FIXED_PARAMETERS[\"batch_size\"] = 16\n",
    "FIXED_PARAMETERS[\"keep_rate\"] = 0.9\n",
    "FIXED_PARAMETERS[\"learning_rate\"] = 0.0004\n",
    "FIXED_PARAMETERS[\"word_embedding_dim\"] = 300\n",
    "FIXED_PARAMETERS[\"hidden_embedding_dim\"] = 300\n",
    "FIXED_PARAMETERS[\"emb_train\"] = True\n",
    "\n",
    "FIXED_PARAMETERS[\"embedding_data_path\"] = the_path + \"glove.840B.300d.txt\"\n",
    "FIXED_PARAMETERS[\"log_path\"] = the_path + \"model/\"\n",
    "FIXED_PARAMETERS[\"ckpt_path\"] = the_path + \"model/\"\n",
    "\n",
    "FIXED_PARAMETERS[\"training_mnli\"] = the_path + \"data/multinli_0.9/multinli_0.9_train.jsonl\"\n",
    "FIXED_PARAMETERS[\"dev_matched\"] = the_path + \"data/multinli_0.9/multinli_0.9_dev_matched.jsonl\"\n",
    "FIXED_PARAMETERS[\"dev_mismatched\"] = the_path + \"data/multinli_0.9/multinli_0.9_dev_mismatched.jsonl\"\n",
    "FIXED_PARAMETERS[\"training_uwre\"] = the_path + \"data/uwre/train.0\"\n",
    "FIXED_PARAMETERS[\"dev_uwre\"] = the_path + \"data/uwre/dev.0\"\n",
    "FIXED_PARAMETERS[\"test_uwre\"] = the_path + \"data/uwre/test.0\"\n",
    "\n",
    "\n",
    "FIXED_PARAMETERS[\"description_num\"] = 15\n",
    "FIXED_PARAMETERS[\"model_type\"] = 'mcim'\n",
    "FIXED_PARAMETERS[\"model_name\"] = 'pretrain_mlp_15'\n",
    "\n",
    "modname = FIXED_PARAMETERS[\"model_name\"] #'pre_train_mlp'\n",
    "logpath = os.path.join(FIXED_PARAMETERS[\"log_path\"], modname) + \".log\"\n",
    "logger = Logger(logpath)\n",
    "\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkkiNav4w1OA"
   },
   "outputs": [],
   "source": [
    "# blocks\n",
    "\"\"\"\n",
    "\n",
    "Functions and components that can be slotted into tensorflow models.\n",
    "\n",
    "TODO: Write functions for various types of attention.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def length(sequence):\n",
    "    \"\"\"\n",
    "    Get true length of sequences (without padding), and mask for true-length in max-length.\n",
    "\n",
    "    Input of shape: (batch_size, max_seq_length, hidden_dim)\n",
    "    Output shapes, \n",
    "    length: (batch_size)\n",
    "    mask: (batch_size, max_seq_length, 1)\n",
    "    \"\"\"\n",
    "    populated = tf.sign(tf.abs(sequence))\n",
    "    length = tf.cast(tf.reduce_sum(populated, axis=1), tf.int32)\n",
    "    mask = tf.cast(tf.expand_dims(populated, -1), tf.float32)\n",
    "    return length, mask\n",
    "\n",
    "\n",
    "def reader(inputs, lengths, output_size, contexts=(None, None), scope=None):\n",
    "    \"\"\"Dynamic bi-LSTM reader; can be conditioned with initial state of other rnn.\n",
    "    Args:\n",
    "        inputs (tensor): The inputs into the bi-LSTM\n",
    "        lengths (tensor): The lengths of the sequences\n",
    "        output_size (int): Size of the LSTM state of the reader.\n",
    "        context (tensor=None, tensor=None): Tuple of initial (forward, backward) states\n",
    "                                  for the LSTM\n",
    "        scope (string): The TensorFlow scope for the reader.\n",
    "        drop_keep_drop (float=1.0): The keep probability for dropout.\n",
    "    Returns:\n",
    "        Outputs (tensor): The outputs from the bi-LSTM.\n",
    "        States (tensor): The cell states from the bi-LSTM.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope or \"reader\") as varscope:\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(output_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            inputs,\n",
    "            sequence_length=lengths,\n",
    "            initial_state_fw=contexts[0],\n",
    "            initial_state_bw=contexts[1],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        return outputs, states\n",
    "\n",
    "\n",
    "def biLSTM(inputs, dim, seq_len, name):\n",
    "    \"\"\"\n",
    "    A Bi-Directional LSTM layer. Returns forward and backward hidden states as a tuple, and cell states as a tuple.\n",
    "\n",
    "    Ouput of hidden states: [(batch_size, max_seq_length, hidden_dim), (batch_size, max_seq_length, hidden_dim)]\n",
    "    Same shape for cell states.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.variable_scope('forward' + name):\n",
    "            lstm_fwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "        with tf.variable_scope('backward' + name):\n",
    "            lstm_bwd = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "\n",
    "        hidden_states, cell_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fwd, cell_bw=lstm_bwd, inputs=inputs, sequence_length=seq_len, dtype=tf.float32, scope=name)\n",
    "\n",
    "    return hidden_states, cell_states\n",
    "\n",
    "\n",
    "def LSTM(inputs, dim, seq_len, name):\n",
    "    \"\"\"\n",
    "    An LSTM layer. Returns hidden states and cell states as a tuple.\n",
    "\n",
    "    Ouput shape of hidden states: (batch_size, max_seq_length, hidden_dim)\n",
    "    Same shape for cell states.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units=dim)\n",
    "        hidden_states, cell_states = tf.nn.dynamic_rnn(cell, inputs=inputs, sequence_length=seq_len, dtype=tf.float32, scope=name)\n",
    "\n",
    "    return hidden_states, cell_states\n",
    "\n",
    "\n",
    "def last_output(output, true_length):\n",
    "    \"\"\"\n",
    "    To get the last hidden layer form a dynamically unrolled RNN.\n",
    "    Input of shape (batch_size, max_seq_length, hidden_dim).\n",
    "\n",
    "    true_length: Tensor of shape (batch_size). Such a tensor is given by the length() function.\n",
    "    Output of shape (batch_size, hidden_dim).\n",
    "    \"\"\"\n",
    "    max_length = int(output.get_shape()[1])\n",
    "    length_mask = tf.expand_dims(tf.one_hot(true_length-1, max_length, on_value=1., off_value=0.), -1)\n",
    "    last_output = tf.reduce_sum(tf.multiply(output, length_mask), 1)\n",
    "    return last_output\n",
    "\n",
    "\n",
    "def masked_softmax(scores, mask):\n",
    "    \"\"\"\n",
    "    Used to calculcate a softmax score with true sequence length (without padding), rather than max-sequence length.\n",
    "\n",
    "    Input shape: (batch_size, max_seq_length, hidden_dim). \n",
    "    mask parameter: Tensor of shape (batch_size, max_seq_length). Such a mask is given by the length() function.\n",
    "    \"\"\"\n",
    "    numerator = tf.exp(tf.subtract(scores, tf.reduce_max(scores, 1, keepdims=True))) * mask\n",
    "    denominator = tf.reduce_sum(numerator, 1, keep_dims=True)\n",
    "    weights = tf.div(numerator, denominator)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flIC-yLTw0q3"
   },
   "outputs": [],
   "source": [
    "# models/msim\n",
    "class msim(object):\n",
    "    def __init__(self, seq_length, emb_dim, hidden_dim, embeddings, emb_train, description_num):\n",
    "        ## Define hyperparameters\n",
    "        self.embedding_dim = emb_dim\n",
    "        self.dim = hidden_dim\n",
    "        self.sequence_length = seq_length\n",
    "        self.description_num = description_num\n",
    "\n",
    "        ## Define the placeholders\n",
    "        self.premise_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.hypothesis_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        self.keep_rate_ph = tf.placeholder(tf.float32, [])\n",
    "\n",
    "        ## Define parameters\n",
    "        self.E = tf.Variable(embeddings, trainable=emb_train)\n",
    "\n",
    "        self.W_mlp = tf.Variable(tf.random_normal([self.dim * 8, self.dim], stddev=0.1))\n",
    "        self.b_mlp = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "\n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.dim, 2], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([2], stddev=0.1))\n",
    "\n",
    "        ## Function for embedding lookup and dropout at embedding layer\n",
    "        def emb_drop(x):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_drop = tf.nn.dropout(emb, self.keep_rate_ph)\n",
    "            return emb_drop\n",
    "\n",
    "        # Get lengths of unpadded sentences\n",
    "        prem_seq_lengths, mask_prem = length(self.premise_x)\n",
    "        hyp_seq_lengths, mask_hyp = length(self.hypothesis_x)\n",
    "\n",
    "\n",
    "        ### First cbiLSTM layer ###\n",
    "        premise_in = emb_drop(self.premise_x)\n",
    "        hypothesis_in = emb_drop(self.hypothesis_x)\n",
    "\n",
    "        hypothesis_outs, c2 = biLSTM(hypothesis_in, dim=self.dim, seq_len=hyp_seq_lengths, name='hypothesis')\n",
    "        # calculate premise based on the condition of hypothesis\n",
    "        with tf.variable_scope(\"conditional_first_premise_layer\") as fstPremise_scope:\n",
    "            premise_outs, c1 = reader(premise_in, prem_seq_lengths, self.dim, c2, scope=fstPremise_scope)\n",
    "\n",
    "        premise_bi = tf.concat(premise_outs, axis=2)\n",
    "        hypothesis_bi = tf.concat(hypothesis_outs, axis=2)\n",
    "\n",
    "        premise_list = tf.unstack(premise_bi, axis=1)\n",
    "        hypothesis_list = tf.unstack(hypothesis_bi, axis=1)\n",
    "\n",
    "\n",
    "        ### Attention ###\n",
    "\n",
    "        scores_all = []\n",
    "        premise_attn = []\n",
    "        alphas = []\n",
    "\n",
    "        for i in range(self.sequence_length):\n",
    "\n",
    "            scores_i_list = []\n",
    "            for j in range(self.sequence_length):\n",
    "                score_ij = tf.reduce_sum(tf.multiply(premise_list[i], hypothesis_list[j]), 1, keep_dims=True)\n",
    "                scores_i_list.append(score_ij)\n",
    "\n",
    "            scores_i = tf.stack(scores_i_list, axis=1)\n",
    "            alpha_i = masked_softmax(scores_i, mask_hyp)\n",
    "            a_tilde_i = tf.reduce_sum(tf.multiply(alpha_i, hypothesis_bi), 1)\n",
    "            premise_attn.append(a_tilde_i)\n",
    "\n",
    "            scores_all.append(scores_i)\n",
    "            alphas.append(alpha_i)\n",
    "\n",
    "        scores_stack = tf.stack(scores_all, axis=2)\n",
    "        scores_list = tf.unstack(scores_stack, axis=1)\n",
    "\n",
    "        hypothesis_attn = []\n",
    "        betas = []\n",
    "        for j in range(self.sequence_length):\n",
    "            scores_j = scores_list[j]\n",
    "            beta_j = masked_softmax(scores_j, mask_prem)\n",
    "            b_tilde_j = tf.reduce_sum(tf.multiply(beta_j, premise_bi), 1)\n",
    "            hypothesis_attn.append(b_tilde_j)\n",
    "\n",
    "            betas.append(beta_j)\n",
    "\n",
    "        # Make attention-weighted sentence representations into one tensor,\n",
    "        premise_attns = tf.stack(premise_attn, axis=1) # (?, 50, 600)\n",
    "        hypothesis_attns = tf.stack(hypothesis_attn, axis=1) # (?, 50, 600)\n",
    "\n",
    "        # For making attention plots,\n",
    "        self.alpha_s = tf.stack(alphas, axis=2) # (?, 50, 50, 1)\n",
    "        self.beta_s = tf.stack(betas, axis=2) # (?, 50, 50, 1)\n",
    "\n",
    "\n",
    "        ### Subcomponent Inference ###\n",
    "\n",
    "        prem_diff = tf.subtract(premise_bi, premise_attns)\n",
    "        prem_mul = tf.multiply(premise_bi, premise_attns)\n",
    "        hyp_diff = tf.subtract(hypothesis_bi, hypothesis_attns)\n",
    "        hyp_mul = tf.multiply(hypothesis_bi, hypothesis_attns)\n",
    "\n",
    "        m_a = tf.concat([premise_bi, premise_attns, prem_diff, prem_mul], 2)\n",
    "        m_b = tf.concat([hypothesis_bi, hypothesis_attns, hyp_diff, hyp_mul], 2) \n",
    "\n",
    "        ### Inference Composition ###\n",
    "\n",
    "        v2_outs, c4 = biLSTM(m_b, dim=self.dim, seq_len=hyp_seq_lengths, name='v2') # hypothesis\n",
    "        # same to hypothesis premise part, calculate v1 based on v2 during Inference Composition\n",
    "        with tf.variable_scope(\"conditional_inference_composition-v1\") as v1_scope:\n",
    "            v1_outs, c3 = reader(m_a, prem_seq_lengths, self.dim, c4, scope=v1_scope) # premise\n",
    "\n",
    "        v1_bi = tf.concat(v1_outs, axis=2) # (?, 50, 600)\n",
    "        v2_bi = tf.concat(v2_outs, axis=2) # (?, 50, 600)\n",
    "\n",
    "\n",
    "        ### Pooling Layer ###\n",
    "        v_1_sum = tf.reduce_sum(v1_bi, 1) # 整列求和 (?, 600) 把每句话的50个单词省略了?\n",
    "        v_1_ave = tf.div(v_1_sum, tf.expand_dims(tf.cast(hyp_seq_lengths, tf.float32), -1)) # (?, 600)\n",
    "\n",
    "        v_2_sum = tf.reduce_sum(v2_bi, 1) # 整列求和 (?, 600)\n",
    "        v_2_ave = tf.div(v_2_sum, tf.expand_dims(tf.cast(hyp_seq_lengths, tf.float32), -1)) # (?, 600)\n",
    "\n",
    "        v_1_max = tf.reduce_max(v1_bi, 1) # 整列求和 (?, 600)\n",
    "        v_2_max = tf.reduce_max(v2_bi, 1) # 整列求和 (?, 600)\n",
    "\n",
    "\n",
    "        v = tf.concat([v_1_ave, v_2_ave, v_1_max, v_2_max], 1)\n",
    "\n",
    "        # MLP layer\n",
    "        h_mlp = tf.nn.tanh(tf.matmul(v, self.W_mlp) + self.b_mlp)\n",
    "        h_fold_mlp = tf.reshape(h_mlp, [-1, self.description_num, self.dim])\n",
    "        h_mean_mlp = tf.reduce_mean(h_fold_mlp, 1)\n",
    "\n",
    "        # Dropout applied to classifier\n",
    "        h_drop = tf.nn.dropout(h_mean_mlp, self.keep_rate_ph)\n",
    "\n",
    "        # Get prediction\n",
    "        self.logits = tf.matmul(h_drop, self.W_cl) + self.b_cl\n",
    "\n",
    "\n",
    "        # Define the cost function\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)) # 一个数字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOwUqqIRBD-B"
   },
   "outputs": [],
   "source": [
    "# data_processing functions\n",
    "\n",
    "def tokenize(string):\n",
    "    string = re.sub(r'\\(|\\)', '', string)\n",
    "    return string.split()\n",
    "  \n",
    "def loadEmbedding_rand(path, word_indices):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings. Doing a random normal initialization for OOV words.\n",
    "    \"\"\"\n",
    "    n = len(word_indices)\n",
    "    m = 300\n",
    "    emb = np.empty((n, m), dtype=np.float32)\n",
    "\n",
    "    emb[:,:] = np.random.normal(size=(n,m))\n",
    "\n",
    "    # Explicitly assign embedding of <PAD> to be zeros.\n",
    "    emb[0:2, :] = np.zeros((1,m), dtype=\"float32\")\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if 300 != None:\n",
    "                if i >= 300:\n",
    "                    break\n",
    "            \n",
    "            s = line.split()\n",
    "            if s[0] in word_indices:\n",
    "                if len(s) > 301:\n",
    "                    tail = s[len(s)-300:]                \n",
    "                    head = [s[0]]\n",
    "                    s = head + tail\n",
    "                    # print(head)\n",
    "                emb[word_indices[s[0]], :] = np.asarray(s[1:]) \n",
    "    return emb\n",
    "\n",
    "def load_nli_data(path):\n",
    "    \"\"\"\n",
    "    Load MultiNLI or SNLI data.\n",
    "    If the \"snli\" parameter is set to True, a genre label of snli will be assigned to the data. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in NLI_LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = NLI_LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "  \n",
    "  \n",
    "def load_uwre_data(path):\n",
    "    \"\"\"\n",
    "    Load UWRE data.\n",
    "    \"uwre\" is set to \"genre\". \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            loaded_example[\"genre\"] = \"uwre\"\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def build_uwre_dictionary(uwre_training_datasets, relation_descriptions):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"\n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(uwre_training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['sentence']))\n",
    "    \n",
    "    for relation in relation_descriptions:\n",
    "        for description in relation_descriptions[relation]:\n",
    "            word_counter.update(tokenize(description))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "\n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices\n",
    "  \n",
    "def build_dictionary(multi_nli_datasets, uwre_training_datasets, relation_descriptions):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"\n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(multi_nli_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['sentence1_binary_parse']))\n",
    "            word_counter.update(tokenize(example['sentence2_binary_parse']))\n",
    "            \n",
    "    for i, dataset in enumerate(uwre_training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['sentence']))\n",
    "    \n",
    "    for relation in relation_descriptions:\n",
    "        for description in relation_descriptions[relation]:\n",
    "            word_counter.update(tokenize(description))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "\n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices\n",
    "  \n",
    "def uwre_sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding.\n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['sentence' + '_index_sequence'] = np.zeros((FIXED_PARAMETERS[\"seq_length\"]), dtype=np.int32)\n",
    "\n",
    "            token_sequence = tokenize(example['sentence'])\n",
    "            padding = FIXED_PARAMETERS[\"seq_length\"] - len(token_sequence)\n",
    "\n",
    "            for i in range(FIXED_PARAMETERS[\"seq_length\"]):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['sentence' + '_index_sequence'][i] = index\n",
    "\n",
    "def descriptions_to_padded_index_sequences(word_indices, relation_descriptions):\n",
    "    padded_relation_descriptions = {}\n",
    "\n",
    "    for relation in relation_descriptions:\n",
    "        descriptions = np.zeros((len(relation_descriptions[relation]), FIXED_PARAMETERS[\"seq_length\"]), dtype=np.int32)\n",
    "        for j,description in enumerate(relation_descriptions[relation]):\n",
    "            token_sequence = tokenize(description)\n",
    "            padding = FIXED_PARAMETERS[\"seq_length\"] - len(token_sequence)\n",
    "\n",
    "            for i in range(FIXED_PARAMETERS[\"seq_length\"]):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[unknown]\n",
    "                descriptions[j][i] = index\n",
    "        padded_relation_descriptions[relation] = descriptions\n",
    "    return padded_relation_descriptions\n",
    "  \n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n",
    "                example[sentence + '_index_sequence'] = np.zeros((FIXED_PARAMETERS[\"seq_length\"]), dtype=np.int32)\n",
    "\n",
    "                token_sequence = tokenize(example[sentence])\n",
    "                padding = FIXED_PARAMETERS[\"seq_length\"] - len(token_sequence)\n",
    "\n",
    "                for i in range(FIXED_PARAMETERS[\"seq_length\"]):\n",
    "                    if i >= len(token_sequence):\n",
    "                        index = word_indices[PADDING]\n",
    "                    else:\n",
    "                        if token_sequence[i] in word_indices:\n",
    "                            index = word_indices[token_sequence[i]]\n",
    "                        else:\n",
    "                            index = word_indices[UNKNOWN]\n",
    "                    example[sentence + '_index_sequence'][i] = index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HX9HYMWqBkZt"
   },
   "outputs": [],
   "source": [
    "training_mnli = load_nli_data(FIXED_PARAMETERS[\"training_mnli\"])\n",
    "dev_matched = load_nli_data(FIXED_PARAMETERS[\"dev_matched\"])\n",
    "dev_mismatched = load_nli_data(FIXED_PARAMETERS[\"dev_mismatched\"])\n",
    "# training_uwre = load_uwre_data(FIXED_PARAMETERS[\"training_uwre\"])\n",
    "dev_uwre = load_uwre_data(FIXED_PARAMETERS[\"dev_uwre\"])\n",
    "# test_uwre = load_uwre_data(FIXED_PARAMETERS[\"test_uwre\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ascOoSyUpMiB"
   },
   "outputs": [],
   "source": [
    "with open(the_path + \"extended_relation_descriptions.json\", 'r') as file:\n",
    "    relation_descriptions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16503,
     "status": "ok",
     "timestamp": 1564256231981,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "gP_2EfxQjqEp",
    "outputId": "ca59d001-a09c-4f50-fd69-e9db7dd70ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load exist dictionary\n"
     ]
    }
   ],
   "source": [
    "dictpath = os.path.join(FIXED_PARAMETERS[\"log_path\"], 'dictionary') + \".p\"\n",
    "\n",
    "if not os.path.isfile(dictpath):\n",
    "  print(\"create new dictionary\")\n",
    "  word_indices = build_dictionary([training_mnli], [training_uwre], relation_descriptions)\n",
    "  pickle.dump(word_indices, open(dictpath, \"wb\"))\n",
    "else:\n",
    "  print(\"load exist dictionary\")\n",
    "  word_indices = pickle.load(open(dictpath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d25sxdMeqYRJ"
   },
   "outputs": [],
   "source": [
    "sentences_to_padded_index_sequences(word_indices, [training_mnli,dev_matched, dev_mismatched])\n",
    "uwre_sentences_to_padded_index_sequences(word_indices, [dev_uwre])\n",
    "padded_relation_descriptions = descriptions_to_padded_index_sequences(word_indices, relation_descriptions)\n",
    "loaded_embeddings = loadEmbedding_rand(FIXED_PARAMETERS[\"embedding_data_path\"], word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh-DDlpp3zV"
   },
   "outputs": [],
   "source": [
    "# class model classifier\n",
    "\n",
    "learning_rate =  FIXED_PARAMETERS[\"learning_rate\"]\n",
    "display_epoch_freq = 1\n",
    "display_step_freq = 50\n",
    "embedding_dim = FIXED_PARAMETERS[\"word_embedding_dim\"]\n",
    "dim = FIXED_PARAMETERS[\"hidden_embedding_dim\"]\n",
    "batch_size = FIXED_PARAMETERS[\"batch_size\"]\n",
    "emb_train = FIXED_PARAMETERS[\"emb_train\"]\n",
    "keep_rate = FIXED_PARAMETERS[\"keep_rate\"]\n",
    "sequence_length = FIXED_PARAMETERS[\"seq_length\"]\n",
    "description_num = int(FIXED_PARAMETERS[\"description_num\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43350,
     "status": "ok",
     "timestamp": 1564256258858,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "8QxN82Hop3Du",
    "outputId": "a8094426-4d9f-43a2-ec0c-898cf3c400e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1] Building model from mlp_esim.py\n"
     ]
    }
   ],
   "source": [
    "logger.Log(\"Building model from %s.py\" %(FIXED_PARAMETERS[\"model_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152302,
     "status": "ok",
     "timestamp": 1564256367822,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "jWMHdvIPuTYG",
    "outputId": "2afecb03-0699-44b4-f913-db112f2559f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 19:37:39.366694 139950971316096 deprecation.py:506] From <ipython-input-7-04d88b008a3e>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0727 19:37:40.378069 139950971316096 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0727 19:37:40.379876 139950971316096 deprecation.py:323] From <ipython-input-6-af3a5211d4f8>:66: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0727 19:37:40.389387 139950971316096 deprecation.py:323] From <ipython-input-6-af3a5211d4f8>:70: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0727 19:37:40.394929 139950971316096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0727 19:37:40.499961 139950971316096 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0727 19:37:40.514932 139950971316096 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0727 19:37:41.949307 139950971316096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0727 19:37:42.436428 139950971316096 deprecation.py:506] From <ipython-input-7-04d88b008a3e>:61: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0727 19:37:42.560263 139950971316096 deprecation.py:323] From <ipython-input-6-af3a5211d4f8>:112: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "my_model = msim(seq_length=sequence_length, emb_dim=embedding_dim,hidden_dim=dim, embeddings=loaded_embeddings,emb_train=emb_train, description_num=description_num)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999).minimize(my_model.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152296,
     "status": "ok",
     "timestamp": 1564256367824,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "PygRZ2mouTP-",
    "outputId": "622adcdd-e90c-46fa-a873-4b17e8185151"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1] Initializing variables\n"
     ]
    }
   ],
   "source": [
    "logger.Log(\"Initializing variables\")\n",
    "init = tf.global_variables_initializer()\n",
    "sess = None\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSu4G5FhyDtb"
   },
   "outputs": [],
   "source": [
    "def get_uwre_minibatch(dataset, start_index, end_index):\n",
    "    indices = range(start_index, end_index)\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "\n",
    "    for i in indices:\n",
    "        relation = dataset[i]['relation']\n",
    "\n",
    "        premise_instance = dataset[i]['sentence_index_sequence']\n",
    "        premise = [premise_instance] * description_num\n",
    "        premise_list.append(premise)\n",
    "\n",
    "        hypothesis_len = len(padded_relation_descriptions[relation])\n",
    "        hypothesis_ind = np.random.choice(hypothesis_len, description_num, replace=False)\n",
    "        hypothesis = padded_relation_descriptions[relation][hypothesis_ind]\n",
    "        hypothesis_list.append(hypothesis)\n",
    "\n",
    "    premise_vectors = np.vstack(premise_list)\n",
    "    hypothesis_vectors = np.vstack(hypothesis_list)\n",
    "    genres = [dataset[i]['genre'] for i in indices]\n",
    "    labels = [dataset[i]['label'] for i in indices]\n",
    "\n",
    "    return premise_vectors, hypothesis_vectors, labels, genres\n",
    "\n",
    "def get_minibatch(dataset, start_index, end_index):\n",
    "    indices = range(start_index, end_index)\n",
    "    premise_vectors = np.vstack([[dataset[i]['sentence1_binary_parse_index_sequence']] * description_num for i in indices])\n",
    "    hypothesis_vectors = np.vstack([[dataset[i]['sentence2_binary_parse_index_sequence']] * description_num for i in indices])\n",
    "    genres = [dataset[i]['genre'] for i in indices]\n",
    "    labels = [dataset[i]['label'] for i in indices]\n",
    "    return premise_vectors, hypothesis_vectors, labels, genres\n",
    "\n",
    "def restore(sess, best=True):\n",
    "    if True:\n",
    "        path = os.path.join(FIXED_PARAMETERS[\"ckpt_path\"], modname) + \".ckpt_best\"\n",
    "    else:\n",
    "        path = os.path.join(FIXED_PARAMETERS[\"ckpt_path\"], modname) + \".ckpt\"\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, path)\n",
    "    logger.Log(\"Model restored from file: %s\" % path)\n",
    "\n",
    "def uwre_classify(model, sess, examples):\n",
    "    # This classifies a list of examples\n",
    "    total_batch = int(len(examples) / batch_size)\n",
    "    logits = np.empty(2)\n",
    "    genres = []\n",
    "    for i in range(total_batch):\n",
    "        minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels, minibatch_genres = get_uwre_minibatch(examples, batch_size * i, batch_size * (i + 1))\n",
    "        feed_dict = {model.premise_x: minibatch_premise_vectors,\n",
    "                            model.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                            model.y: minibatch_labels,\n",
    "                            model.keep_rate_ph: 1.0}\n",
    "        genres += minibatch_genres\n",
    "        logit, cost = sess.run([model.logits, model.total_cost], feed_dict)\n",
    "        logits = np.vstack([logits, logit])\n",
    "    return genres, np.argmax(logits[1:], axis=1), cost\n",
    "\n",
    "def classify(model, sess, examples):\n",
    "    # This classifies a list of examples\n",
    "    total_batch = int(len(examples) / batch_size)\n",
    "    logits = np.empty(2)\n",
    "    genres = []\n",
    "    for i in range(total_batch):\n",
    "        minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels, minibatch_genres = get_minibatch(examples,\n",
    "                                batch_size * i, batch_size * (i + 1))\n",
    "        feed_dict = {model.premise_x: minibatch_premise_vectors,\n",
    "                            model.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                            model.y: minibatch_labels,\n",
    "                            model.keep_rate_ph: 1.0}\n",
    "        genres += minibatch_genres\n",
    "        logit, cost = sess.run([model.logits, model.total_cost], feed_dict)\n",
    "        logits = np.vstack([logits, logit])\n",
    "\n",
    "    return genres, np.argmax(logits[1:], axis=1), cost\n",
    "  \n",
    "def evaluate_classifier(classifier, model, sess, eval_set, batch_size):\n",
    "    \"\"\"\n",
    "    Function to get accuracy and cost of the model, evaluated on a chosen dataset.\n",
    "\n",
    "    classifier: the model's classfier, it should return genres, logit values, and cost for a given minibatch of the evaluation dataset\n",
    "    eval_set: the chosen evaluation set, for eg. the dev-set\n",
    "    batch_size: the size of minibatches.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    genres, hypotheses, cost = classifier(model, sess, eval_set)\n",
    "    cost = cost / batch_size\n",
    "    full_batch = int(len(eval_set) / batch_size) * batch_size\n",
    "    for i in range(full_batch):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == eval_set[i]['label']:\n",
    "            correct += 1\n",
    "    return correct / float(len(eval_set)), cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 782620,
     "status": "ok",
     "timestamp": 1564256998164,
     "user": {
      "displayName": "Haining Tong",
      "photoUrl": "https://lh4.googleusercontent.com/-lR-MdhDQI0s/AAAAAAAAAAI/AAAAAAAAAH0/LtM9E34-pSI/s64/photo.jpg",
      "userId": "13855022567936776467"
     },
     "user_tz": -120
    },
    "id": "yCySRUwk1iXE",
    "outputId": "9cdc7341-fa91-46f1-9248-8d957f3e31df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 19:39:35.955064 139950971316096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "[1] Restored best matched-dev acc: 0.685894\n",
      " Restored best mismatched-dev acc: 0.695808\n",
      "                    Restored best MulitNLI train acc: 0.689200\n",
      "[1] Model restored from file: /content/gdrive/My Drive/master_thesis/model/pretrain_mlp_15.ckpt\n",
      "[1] Start Training...\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "step = 0\n",
    "epoch = 0\n",
    "best_dev_mat = 0.\n",
    "best_mtrain_acc = 0.\n",
    "last_train_acc = [.001, .001, .001, .001, .001]\n",
    "best_step = 0\n",
    "\n",
    "dev_mat = dev_matched\n",
    "dev_mismat = dev_mismatched\n",
    "\n",
    "# Restore most recent checkpoint if it exists. \n",
    "# Also restore values for best dev-set accuracy and best training-set accuracy\n",
    "ckpt_file = os.path.join(FIXED_PARAMETERS[\"ckpt_path\"], modname) + \".ckpt\"\n",
    "if os.path.isfile(ckpt_file + \".meta\"):\n",
    "    if os.path.isfile(ckpt_file + \"_best.meta\"):\n",
    "        saver.restore(sess, (ckpt_file + \"_best\"))\n",
    "        best_dev_mat, dev_cost_mat = evaluate_classifier(classify, my_model, sess, dev_mat, batch_size)\n",
    "        best_dev_mismat, dev_cost_mismat = evaluate_classifier(classify, my_model, sess, dev_mismat, batch_size)\n",
    "        best_mtrain_acc, mtrain_cost = evaluate_classifier(classify, my_model, sess, training_mnli[0:5000], batch_size)\n",
    "        logger.Log(\"Restored best matched-dev acc: %f\\n Restored best mismatched-dev acc: %f\\n \\\n",
    "                   Restored best MulitNLI train acc: %f\" %(best_dev_mat, best_dev_mismat, best_mtrain_acc))\n",
    "\n",
    "\n",
    "    saver.restore(sess, ckpt_file)\n",
    "    logger.Log(\"Model restored from file: %s\" % ckpt_file)\n",
    "\n",
    "# Combine MultiNLI and SNLI data. Alpha has a default value of 0, if we want to use SNLI data, it must be passed as an argument.\n",
    "\n",
    "### Training cycle\n",
    "logger.Log(\"Start Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BG68zPILyhwT",
    "outputId": "8d77e7e4-8815-4787-91a6-ecded181119a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1] Step: 0\t uwre dev acc: 0.492437\t uwre dev cost 0.044546\t\n",
      "[1] Step: 0\t Dev-matched acc: 0.688434\t Dev-mismatched acc: 0.696255\t                     MultiNLI train acc: 0.701800\n",
      "[1] Step: 0\t Dev-matched cost: 0.031693\t Dev-mismatched cost: 0.032678\t                     MultiNLI train cost: 0.039820\n",
      "[1] Checkpointing with new best matched-dev accuracy: 0.688434\n",
      "[1] Step: 50\t uwre dev acc: 0.484034\t uwre dev cost 0.048499\t\n",
      "[1] Step: 50\t Dev-matched acc: 0.665571\t Dev-mismatched acc: 0.681486\t                     MultiNLI train acc: 0.727400\n",
      "[1] Step: 50\t Dev-matched cost: 0.040901\t Dev-mismatched cost: 0.028750\t                     MultiNLI train cost: 0.043138\n",
      "[1] Step: 100\t uwre dev acc: 0.490756\t uwre dev cost 0.065204\t\n",
      "[1] Step: 100\t Dev-matched acc: 0.643156\t Dev-mismatched acc: 0.643742\t                     MultiNLI train acc: 0.726600\n",
      "[1] Step: 100\t Dev-matched cost: 0.083094\t Dev-mismatched cost: 0.028649\t                     MultiNLI train cost: 0.051149\n",
      "[1] Step: 150\t uwre dev acc: 0.494118\t uwre dev cost 0.065124\t\n",
      "[1] Step: 150\t Dev-matched acc: 0.654961\t Dev-mismatched acc: 0.667164\t                     MultiNLI train acc: 0.788800\n",
      "[1] Step: 150\t Dev-matched cost: 0.060505\t Dev-mismatched cost: 0.024712\t                     MultiNLI train cost: 0.057009\n",
      "[1] Step: 200\t uwre dev acc: 0.473950\t uwre dev cost 0.123716\t\n",
      "[1] Step: 200\t Dev-matched acc: 0.625822\t Dev-mismatched acc: 0.641504\t                     MultiNLI train acc: 0.786600\n",
      "[1] Step: 200\t Dev-matched cost: 0.053840\t Dev-mismatched cost: 0.031276\t                     MultiNLI train cost: 0.064395\n",
      "[1] Step: 250\t uwre dev acc: 0.490756\t uwre dev cost 0.082958\t\n",
      "[1] Step: 250\t Dev-matched acc: 0.621339\t Dev-mismatched acc: 0.628972\t                     MultiNLI train acc: 0.787200\n",
      "[1] Step: 250\t Dev-matched cost: 0.064521\t Dev-mismatched cost: 0.048535\t                     MultiNLI train cost: 0.045913\n",
      "[1] Step: 300\t uwre dev acc: 0.472269\t uwre dev cost 0.091579\t\n",
      "[1] Step: 300\t Dev-matched acc: 0.646294\t Dev-mismatched acc: 0.665523\t                     MultiNLI train acc: 0.805800\n",
      "[1] Step: 300\t Dev-matched cost: 0.044610\t Dev-mismatched cost: 0.043402\t                     MultiNLI train cost: 0.021624\n",
      "[1] Step: 350\t uwre dev acc: 0.487395\t uwre dev cost 0.135342\t\n",
      "[1] Step: 350\t Dev-matched acc: 0.614614\t Dev-mismatched acc: 0.630762\t                     MultiNLI train acc: 0.761200\n",
      "[1] Step: 350\t Dev-matched cost: 0.074592\t Dev-mismatched cost: 0.034036\t                     MultiNLI train cost: 0.012870\n",
      "[1] Step: 400\t uwre dev acc: 0.478992\t uwre dev cost 0.121481\t\n",
      "[1] Step: 400\t Dev-matched acc: 0.643007\t Dev-mismatched acc: 0.649709\t                     MultiNLI train acc: 0.769600\n",
      "[1] Step: 400\t Dev-matched cost: 0.045324\t Dev-mismatched cost: 0.030103\t                     MultiNLI train cost: 0.023829\n",
      "[1] Step: 450\t uwre dev acc: 0.485714\t uwre dev cost 0.091744\t\n",
      "[1] Step: 450\t Dev-matched acc: 0.645846\t Dev-mismatched acc: 0.647620\t                     MultiNLI train acc: 0.769800\n",
      "[1] Step: 450\t Dev-matched cost: 0.061107\t Dev-mismatched cost: 0.019019\t                     MultiNLI train cost: 0.029578\n",
      "[1] Step: 500\t uwre dev acc: 0.447059\t uwre dev cost 0.096355\t\n",
      "[1] Step: 500\t Dev-matched acc: 0.651823\t Dev-mismatched acc: 0.656721\t                     MultiNLI train acc: 0.779600\n",
      "[1] Step: 500\t Dev-matched cost: 0.052272\t Dev-mismatched cost: 0.032282\t                     MultiNLI train cost: 0.048531\n",
      "[1] Step: 550\t uwre dev acc: 0.468908\t uwre dev cost 0.065395\t\n",
      "[1] Step: 550\t Dev-matched acc: 0.654065\t Dev-mismatched acc: 0.664180\t                     MultiNLI train acc: 0.735400\n",
      "[1] Step: 550\t Dev-matched cost: 0.039393\t Dev-mismatched cost: 0.037463\t                     MultiNLI train cost: 0.035785\n",
      "[1] Step: 600\t uwre dev acc: 0.482353\t uwre dev cost 0.060666\t\n",
      "[1] Step: 600\t Dev-matched acc: 0.653467\t Dev-mismatched acc: 0.666716\t                     MultiNLI train acc: 0.722200\n",
      "[1] Step: 600\t Dev-matched cost: 0.038503\t Dev-mismatched cost: 0.035463\t                     MultiNLI train cost: 0.036956\n",
      "[1] Step: 650\t uwre dev acc: 0.494118\t uwre dev cost 0.058979\t\n",
      "[1] Step: 650\t Dev-matched acc: 0.679319\t Dev-mismatched acc: 0.688349\t                     MultiNLI train acc: 0.762800\n",
      "[1] Step: 650\t Dev-matched cost: 0.038780\t Dev-mismatched cost: 0.031805\t                     MultiNLI train cost: 0.031504\n",
      "[1] Step: 700\t uwre dev acc: 0.485714\t uwre dev cost 0.060890\t\n",
      "[1] Step: 700\t Dev-matched acc: 0.681261\t Dev-mismatched acc: 0.688349\t                     MultiNLI train acc: 0.766400\n",
      "[1] Step: 700\t Dev-matched cost: 0.038905\t Dev-mismatched cost: 0.026378\t                     MultiNLI train cost: 0.030240\n",
      "[1] Step: 750\t uwre dev acc: 0.489076\t uwre dev cost 0.062654\t\n",
      "[1] Step: 750\t Dev-matched acc: 0.685894\t Dev-mismatched acc: 0.696106\t                     MultiNLI train acc: 0.746000\n",
      "[1] Step: 750\t Dev-matched cost: 0.034755\t Dev-mismatched cost: 0.028408\t                     MultiNLI train cost: 0.032389\n",
      "[1] Step: 800\t uwre dev acc: 0.489076\t uwre dev cost 0.057446\t\n",
      "[1] Step: 800\t Dev-matched acc: 0.682158\t Dev-mismatched acc: 0.689691\t                     MultiNLI train acc: 0.756800\n",
      "[1] Step: 800\t Dev-matched cost: 0.041845\t Dev-mismatched cost: 0.027563\t                     MultiNLI train cost: 0.034410\n",
      "[1] Step: 850\t uwre dev acc: 0.484034\t uwre dev cost 0.054569\t\n",
      "[1] Step: 850\t Dev-matched acc: 0.660341\t Dev-mismatched acc: 0.678204\t                     MultiNLI train acc: 0.779800\n",
      "[1] Step: 850\t Dev-matched cost: 0.049503\t Dev-mismatched cost: 0.024771\t                     MultiNLI train cost: 0.034797\n",
      "[1] Step: 900\t uwre dev acc: 0.473950\t uwre dev cost 0.057905\t\n",
      "[1] Step: 900\t Dev-matched acc: 0.677227\t Dev-mismatched acc: 0.688498\t                     MultiNLI train acc: 0.748200\n",
      "[1] Step: 900\t Dev-matched cost: 0.038367\t Dev-mismatched cost: 0.031459\t                     MultiNLI train cost: 0.039912\n",
      "[1] Step: 950\t uwre dev acc: 0.485714\t uwre dev cost 0.055656\t\n",
      "[1] Step: 950\t Dev-matched acc: 0.677675\t Dev-mismatched acc: 0.690885\t                     MultiNLI train acc: 0.775400\n",
      "[1] Step: 950\t Dev-matched cost: 0.039253\t Dev-mismatched cost: 0.031035\t                     MultiNLI train cost: 0.031868\n",
      "[1] Step: 1000\t uwre dev acc: 0.485714\t uwre dev cost 0.061921\t\n",
      "[1] Step: 1000\t Dev-matched acc: 0.684549\t Dev-mismatched acc: 0.691183\t                     MultiNLI train acc: 0.762400\n",
      "[1] Step: 1000\t Dev-matched cost: 0.038299\t Dev-mismatched cost: 0.027281\t                     MultiNLI train cost: 0.036336\n",
      "[1] Step: 1050\t uwre dev acc: 0.492437\t uwre dev cost 0.068485\t\n",
      "[1] Step: 1050\t Dev-matched acc: 0.672594\t Dev-mismatched acc: 0.693719\t                     MultiNLI train acc: 0.776800\n",
      "[1] Step: 1050\t Dev-matched cost: 0.053521\t Dev-mismatched cost: 0.024851\t                     MultiNLI train cost: 0.033289\n",
      "[1] Step: 1100\t uwre dev acc: 0.485714\t uwre dev cost 0.065420\t\n",
      "[1] Step: 1100\t Dev-matched acc: 0.684399\t Dev-mismatched acc: 0.696703\t                     MultiNLI train acc: 0.776400\n",
      "[1] Step: 1100\t Dev-matched cost: 0.036910\t Dev-mismatched cost: 0.029032\t                     MultiNLI train cost: 0.035993\n",
      "[1] Step: 1150\t uwre dev acc: 0.482353\t uwre dev cost 0.063609\t\n",
      "[1] Step: 1150\t Dev-matched acc: 0.684399\t Dev-mismatched acc: 0.688945\t                     MultiNLI train acc: 0.767800\n",
      "[1] Step: 1150\t Dev-matched cost: 0.038627\t Dev-mismatched cost: 0.034658\t                     MultiNLI train cost: 0.034881\n",
      "[1] Step: 1200\t uwre dev acc: 0.482353\t uwre dev cost 0.056706\t\n",
      "[1] Step: 1200\t Dev-matched acc: 0.684997\t Dev-mismatched acc: 0.694764\t                     MultiNLI train acc: 0.769200\n",
      "[1] Step: 1200\t Dev-matched cost: 0.036203\t Dev-mismatched cost: 0.034289\t                     MultiNLI train cost: 0.036788\n",
      "[1] Step: 1250\t uwre dev acc: 0.475630\t uwre dev cost 0.067664\t\n",
      "[1] Step: 1250\t Dev-matched acc: 0.680813\t Dev-mismatched acc: 0.693868\t                     MultiNLI train acc: 0.776000\n",
      "[1] Step: 1250\t Dev-matched cost: 0.044961\t Dev-mismatched cost: 0.035479\t                     MultiNLI train cost: 0.035888\n",
      "[1] Step: 1300\t uwre dev acc: 0.495798\t uwre dev cost 0.058659\t\n",
      "[1] Step: 1300\t Dev-matched acc: 0.685445\t Dev-mismatched acc: 0.697598\t                     MultiNLI train acc: 0.741800\n",
      "[1] Step: 1300\t Dev-matched cost: 0.039259\t Dev-mismatched cost: 0.033486\t                     MultiNLI train cost: 0.034391\n",
      "[1] Step: 1350\t uwre dev acc: 0.494118\t uwre dev cost 0.061271\t\n",
      "[1] Step: 1350\t Dev-matched acc: 0.682756\t Dev-mismatched acc: 0.700283\t                     MultiNLI train acc: 0.768600\n",
      "[1] Step: 1350\t Dev-matched cost: 0.036835\t Dev-mismatched cost: 0.030619\t                     MultiNLI train cost: 0.033568\n",
      "[1] Step: 1400\t uwre dev acc: 0.494118\t uwre dev cost 0.056980\t\n",
      "[1] Step: 1400\t Dev-matched acc: 0.679916\t Dev-mismatched acc: 0.697151\t                     MultiNLI train acc: 0.760800\n",
      "[1] Step: 1400\t Dev-matched cost: 0.039201\t Dev-mismatched cost: 0.031896\t                     MultiNLI train cost: 0.034760\n",
      "[1] Step: 1450\t uwre dev acc: 0.490756\t uwre dev cost 0.057496\t\n",
      "[1] Step: 1450\t Dev-matched acc: 0.685595\t Dev-mismatched acc: 0.694913\t                     MultiNLI train acc: 0.737200\n",
      "[1] Step: 1450\t Dev-matched cost: 0.036580\t Dev-mismatched cost: 0.032804\t                     MultiNLI train cost: 0.039042\n",
      "[1] Step: 1500\t uwre dev acc: 0.482353\t uwre dev cost 0.067947\t\n",
      "[1] Step: 1500\t Dev-matched acc: 0.670353\t Dev-mismatched acc: 0.695957\t                     MultiNLI train acc: 0.781000\n",
      "[1] Step: 1500\t Dev-matched cost: 0.046981\t Dev-mismatched cost: 0.026740\t                     MultiNLI train cost: 0.039413\n",
      "[1] Step: 1550\t uwre dev acc: 0.482353\t uwre dev cost 0.071010\t\n",
      "[1] Step: 1550\t Dev-matched acc: 0.683503\t Dev-mismatched acc: 0.703267\t                     MultiNLI train acc: 0.765800\n",
      "[1] Step: 1550\t Dev-matched cost: 0.043262\t Dev-mismatched cost: 0.025963\t                     MultiNLI train cost: 0.041153\n",
      "[1] Step: 1600\t uwre dev acc: 0.482353\t uwre dev cost 0.052324\t\n",
      "[1] Step: 1600\t Dev-matched acc: 0.659594\t Dev-mismatched acc: 0.682679\t                     MultiNLI train acc: 0.766800\n",
      "[1] Step: 1600\t Dev-matched cost: 0.041582\t Dev-mismatched cost: 0.028296\t                     MultiNLI train cost: 0.037247\n",
      "[1] Step: 1650\t uwre dev acc: 0.497479\t uwre dev cost 0.060218\t\n",
      "[1] Step: 1650\t Dev-matched acc: 0.672445\t Dev-mismatched acc: 0.691631\t                     MultiNLI train acc: 0.767600\n",
      "[1] Step: 1650\t Dev-matched cost: 0.044835\t Dev-mismatched cost: 0.027415\t                     MultiNLI train cost: 0.040829\n",
      "[1] Step: 1700\t uwre dev acc: 0.489076\t uwre dev cost 0.061596\t\n",
      "[1] Step: 1700\t Dev-matched acc: 0.679169\t Dev-mismatched acc: 0.696255\t                     MultiNLI train acc: 0.730400\n",
      "[1] Step: 1700\t Dev-matched cost: 0.036717\t Dev-mismatched cost: 0.032652\t                     MultiNLI train cost: 0.042558\n",
      "[1] Step: 1750\t uwre dev acc: 0.485714\t uwre dev cost 0.059978\t\n",
      "[1] Step: 1750\t Dev-matched acc: 0.688434\t Dev-mismatched acc: 0.702223\t                     MultiNLI train acc: 0.758000\n",
      "[1] Step: 1750\t Dev-matched cost: 0.036558\t Dev-mismatched cost: 0.026684\t                     MultiNLI train cost: 0.044093\n",
      "[1] Step: 1800\t uwre dev acc: 0.494118\t uwre dev cost 0.057119\t\n",
      "[1] Step: 1800\t Dev-matched acc: 0.677077\t Dev-mismatched acc: 0.690586\t                     MultiNLI train acc: 0.707600\n",
      "[1] Step: 1800\t Dev-matched cost: 0.036089\t Dev-mismatched cost: 0.030681\t                     MultiNLI train cost: 0.049459\n",
      "[1] Step: 1850\t uwre dev acc: 0.492437\t uwre dev cost 0.062681\t\n",
      "[1] Step: 1850\t Dev-matched acc: 0.680514\t Dev-mismatched acc: 0.694913\t                     MultiNLI train acc: 0.705800\n",
      "[1] Step: 1850\t Dev-matched cost: 0.031467\t Dev-mismatched cost: 0.032147\t                     MultiNLI train cost: 0.052058\n",
      "[1] Step: 1900\t uwre dev acc: 0.497479\t uwre dev cost 0.054599\t\n",
      "[1] Step: 1900\t Dev-matched acc: 0.654363\t Dev-mismatched acc: 0.677607\t                     MultiNLI train acc: 0.745800\n",
      "[1] Step: 1900\t Dev-matched cost: 0.037228\t Dev-mismatched cost: 0.027960\t                     MultiNLI train cost: 0.038366\n",
      "[1] Step: 1950\t uwre dev acc: 0.494118\t uwre dev cost 0.059457\t\n",
      "[1] Step: 1950\t Dev-matched acc: 0.674387\t Dev-mismatched acc: 0.687155\t                     MultiNLI train acc: 0.769000\n",
      "[1] Step: 1950\t Dev-matched cost: 0.037658\t Dev-mismatched cost: 0.024086\t                     MultiNLI train cost: 0.046755\n",
      "[1] Step: 2000\t uwre dev acc: 0.487395\t uwre dev cost 0.059383\t\n",
      "[1] Step: 2000\t Dev-matched acc: 0.658249\t Dev-mismatched acc: 0.673281\t                     MultiNLI train acc: 0.757400\n",
      "[1] Step: 2000\t Dev-matched cost: 0.041886\t Dev-mismatched cost: 0.024061\t                     MultiNLI train cost: 0.040837\n",
      "[1] Step: 2050\t uwre dev acc: 0.489076\t uwre dev cost 0.060624\t\n",
      "[1] Step: 2050\t Dev-matched acc: 0.690078\t Dev-mismatched acc: 0.702820\t                     MultiNLI train acc: 0.736600\n",
      "[1] Step: 2050\t Dev-matched cost: 0.036469\t Dev-mismatched cost: 0.030056\t                     MultiNLI train cost: 0.043084\n",
      "[1] Step: 2100\t uwre dev acc: 0.494118\t uwre dev cost 0.073626\t\n",
      "[1] Step: 2100\t Dev-matched acc: 0.692618\t Dev-mismatched acc: 0.704162\t                     MultiNLI train acc: 0.737400\n",
      "[1] Step: 2100\t Dev-matched cost: 0.037191\t Dev-mismatched cost: 0.033336\t                     MultiNLI train cost: 0.046950\n",
      "[1] Step: 2150\t uwre dev acc: 0.499160\t uwre dev cost 0.062122\t\n",
      "[1] Step: 2150\t Dev-matched acc: 0.687089\t Dev-mismatched acc: 0.702074\t                     MultiNLI train acc: 0.770400\n",
      "[1] Step: 2150\t Dev-matched cost: 0.041299\t Dev-mismatched cost: 0.028547\t                     MultiNLI train cost: 0.038221\n",
      "[1] Step: 2200\t uwre dev acc: 0.487395\t uwre dev cost 0.065512\t\n",
      "[1] Step: 2200\t Dev-matched acc: 0.667215\t Dev-mismatched acc: 0.681188\t                     MultiNLI train acc: 0.762200\n",
      "[1] Step: 2200\t Dev-matched cost: 0.044601\t Dev-mismatched cost: 0.028606\t                     MultiNLI train cost: 0.036309\n",
      "[1] Step: 2250\t uwre dev acc: 0.490756\t uwre dev cost 0.064539\t\n",
      "[1] Step: 2250\t Dev-matched acc: 0.680663\t Dev-mismatched acc: 0.692824\t                     MultiNLI train acc: 0.757000\n",
      "[1] Step: 2250\t Dev-matched cost: 0.045917\t Dev-mismatched cost: 0.027019\t                     MultiNLI train cost: 0.041275\n",
      "[1] Step: 2300\t uwre dev acc: 0.492437\t uwre dev cost 0.074518\t\n",
      "[1] Step: 2300\t Dev-matched acc: 0.674238\t Dev-mismatched acc: 0.685514\t                     MultiNLI train acc: 0.757200\n",
      "[1] Step: 2300\t Dev-matched cost: 0.041208\t Dev-mismatched cost: 0.031804\t                     MultiNLI train cost: 0.041748\n",
      "[1] Step: 2350\t uwre dev acc: 0.487395\t uwre dev cost 0.065944\t\n",
      "[1] Step: 2350\t Dev-matched acc: 0.696952\t Dev-mismatched acc: 0.714755\t                     MultiNLI train acc: 0.754400\n",
      "[1] Step: 2350\t Dev-matched cost: 0.035267\t Dev-mismatched cost: 0.030927\t                     MultiNLI train cost: 0.046118\n",
      "[1] Step: 2400\t uwre dev acc: 0.485714\t uwre dev cost 0.054791\t\n",
      "[1] Step: 2400\t Dev-matched acc: 0.687836\t Dev-mismatched acc: 0.699985\t                     MultiNLI train acc: 0.739800\n",
      "[1] Step: 2400\t Dev-matched cost: 0.030065\t Dev-mismatched cost: 0.030612\t                     MultiNLI train cost: 0.042333\n",
      "[1] Step: 2450\t uwre dev acc: 0.499160\t uwre dev cost 0.050276\t\n",
      "[1] Step: 2450\t Dev-matched acc: 0.674387\t Dev-mismatched acc: 0.694614\t                     MultiNLI train acc: 0.747400\n",
      "[1] Step: 2450\t Dev-matched cost: 0.034737\t Dev-mismatched cost: 0.030663\t                     MultiNLI train cost: 0.042236\n",
      "[1] Step: 2500\t uwre dev acc: 0.470588\t uwre dev cost 0.050279\t\n",
      "[1] Step: 2500\t Dev-matched acc: 0.678571\t Dev-mismatched acc: 0.697449\t                     MultiNLI train acc: 0.747600\n",
      "[1] Step: 2500\t Dev-matched cost: 0.031926\t Dev-mismatched cost: 0.032157\t                     MultiNLI train cost: 0.038228\n",
      "[1] Step: 2550\t uwre dev acc: 0.494118\t uwre dev cost 0.059603\t\n",
      "[1] Step: 2550\t Dev-matched acc: 0.694860\t Dev-mismatched acc: 0.708787\t                     MultiNLI train acc: 0.722000\n",
      "[1] Step: 2550\t Dev-matched cost: 0.026531\t Dev-mismatched cost: 0.035382\t                     MultiNLI train cost: 0.045448\n",
      "[1] Step: 2600\t uwre dev acc: 0.490756\t uwre dev cost 0.046846\t\n",
      "[1] Step: 2600\t Dev-matched acc: 0.693963\t Dev-mismatched acc: 0.713114\t                     MultiNLI train acc: 0.740200\n",
      "[1] Step: 2600\t Dev-matched cost: 0.031083\t Dev-mismatched cost: 0.033656\t                     MultiNLI train cost: 0.038985\n",
      "[1] Step: 2650\t uwre dev acc: 0.487395\t uwre dev cost 0.050957\t\n",
      "[1] Step: 2650\t Dev-matched acc: 0.689779\t Dev-mismatched acc: 0.710130\t                     MultiNLI train acc: 0.757400\n",
      "[1] Step: 2650\t Dev-matched cost: 0.030259\t Dev-mismatched cost: 0.034513\t                     MultiNLI train cost: 0.038075\n",
      "[1] Step: 2700\t uwre dev acc: 0.478992\t uwre dev cost 0.048638\t\n",
      "[1] Step: 2700\t Dev-matched acc: 0.698745\t Dev-mismatched acc: 0.712815\t                     MultiNLI train acc: 0.763800\n",
      "[1] Step: 2700\t Dev-matched cost: 0.030523\t Dev-mismatched cost: 0.034685\t                     MultiNLI train cost: 0.038354\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    training_data = training_mnli\n",
    "    random.shuffle(training_data)\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(len(training_data) / batch_size)\n",
    "\n",
    "    # Loop over all batches in epoch\n",
    "    for i in range(total_batch):\n",
    "        # Assemble a minibatch of the next B examples\n",
    "        minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels, minibatch_genres = get_minibatch(\n",
    "            training_data, batch_size * i, batch_size * (i + 1))\n",
    "\n",
    "        # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "        # cost function for logging\n",
    "        feed_dict = {my_model.premise_x: minibatch_premise_vectors,\n",
    "                        my_model.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                        my_model.y: minibatch_labels,\n",
    "                        my_model.keep_rate_ph: keep_rate}\n",
    "        _, c = sess.run([optimizer, my_model.total_cost], feed_dict)\n",
    "\n",
    "        # Since a single epoch can take a  ages for larger models (ESIM),\n",
    "        # we'll print  accuracy every 50 steps\n",
    "        if step % display_step_freq == 0:\n",
    "            dev_acc_mat, dev_cost_mat = evaluate_classifier(classify, my_model, sess, dev_mat, batch_size)\n",
    "            dev_acc_mismat, dev_cost_mismat = evaluate_classifier(classify, my_model, sess, dev_mismat, batch_size)\n",
    "\n",
    "            dev_acc_uwre, dev_cost_uwre = evaluate_classifier(uwre_classify, my_model, sess, dev_uwre, batch_size)\n",
    "\n",
    "            mtrain_acc, mtrain_cost = evaluate_classifier(classify, my_model, sess, training_mnli[0:5000], batch_size)\n",
    "\n",
    "            logger.Log(\"Step: %i\\t uwre dev acc: %f\\t uwre dev cost %f\\t\" \\\n",
    "                    %(step, dev_acc_uwre, dev_cost_uwre))\n",
    "            logger.Log(\"Step: %i\\t Dev-matched acc: %f\\t Dev-mismatched acc: %f\\t \\\n",
    "                    MultiNLI train acc: %f\" %(step, dev_acc_mat,\n",
    "                        dev_acc_mismat, mtrain_acc))\n",
    "            logger.Log(\"Step: %i\\t Dev-matched cost: %f\\t Dev-mismatched cost: %f\\t \\\n",
    "                    MultiNLI train cost: %f\" %(step, dev_cost_mat,\n",
    "                        dev_cost_mismat, mtrain_cost))\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            saver.save(sess, ckpt_file)\n",
    "            best_test = 100 * (1 - best_dev_mat / dev_acc_mat)\n",
    "            if best_test > 0.04:\n",
    "                saver.save(sess, ckpt_file + \"_best\")\n",
    "                best_dev_mat = dev_acc_mat\n",
    "                best_mtrain_acc = mtrain_acc\n",
    "                best_step = step\n",
    "                logger.Log(\"Checkpointing with new best matched-dev accuracy: %f\" %(best_dev_mat))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / (total_batch * batch_size)\n",
    "\n",
    "    # Display some statistics about the epoch\n",
    "    if epoch % display_epoch_freq == 0:\n",
    "        logger.Log(\"Epoch: %i\\t Avg. Cost: %f\" %(epoch+1, avg_cost))\n",
    "\n",
    "    epoch += 1\n",
    "    last_train_acc[(epoch % 5) - 1] = mtrain_acc\n",
    "\n",
    "    # Early stopping\n",
    "    progress = 1000 * (sum(last_train_acc)/(5 * min(last_train_acc)) - 1)\n",
    "\n",
    "    if (progress < 0.1) or (step > best_step + 30000):\n",
    "        logger.Log(\"Best matched-dev accuracy: %s\" %(best_dev_mat))\n",
    "        logger.Log(\"MultiNLI Train accuracy: %s\" %(best_mtrain_acc))\n",
    "        completed = True\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6JQmikh92vF"
   },
   "outputs": [],
   "source": [
    "saver.save(sess, ckpt_file + \"_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxER4zDV-GEw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcQyXDXGJwYN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IA1tuZGEJwWG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lzn5pILFoMY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqXogkEZFoFj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMszYX1C0N36"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGONj3CHOUh2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdDR2YU58YoO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tafrsGuQ8Ygn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8E6YuuK8YXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbDDiJ09C01J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW7HDOlQB4lX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZRmYUCme_WG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pre_Train.ipynb",
   "provenance": [
    {
     "file_id": "1dA7ZEjDL_qW8x_4VhCQqNPQ6xwkZXD8J",
     "timestamp": 1562172891179
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
